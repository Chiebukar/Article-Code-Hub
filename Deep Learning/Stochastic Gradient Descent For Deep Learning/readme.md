# Stochastic Gradient Descent
Stochastic gradient descent is an optimization algorithm that speeds up the convergence of deep learning models. It applies multiple weight updates for the parameters of the model in every iteration according to the number of data points. By so doing, SGD improves efficiency and reduces the time of training models with a huge amount of data.

While the basic SGD method could become computationally expensive, the mini-batch SGD method applies SGD using batches of the data points, thereby reducing the computations incurred when using the basic SGD. The steps, pace and movement of the SGD towards the global minima are controlled using the learning rate, Momentum and Nesterov acceleration hyperparameters respectively.

Read the full article [here](https://samuel-ozechi.medium.com/stochastic-gradient-descent-for-deep-learning-8d911b6b625a)
